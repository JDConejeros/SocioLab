---
title: "Introducción al análisis de redes sociales"
subtitle: "Laboratorio de datos sociales UDP"
author: 
  - José Conejeros <br> <jose.conejeros1@mail.udp.cl>
  - Fernanda Hurtado <br> <fernanda.hurtado@mail.udp.cl>
  - Patricio Alarcón <br> <patricio.alarcon@mail.udp.cl>
date: last-modified
date-format: 'dddd DD [de] MMMM, YYYY'
last-modified:
title-block-banner: true
format: 
  html:
    css: "files/style.css"
    page-layout: full
    embed-resources: true
    smooth-scroll: true
    fontcolor: black
    toc: true
    toc-location: left
    toc-title: Indice
    code-copy: true
    code-link: true
    code-fold: show
    code-tools: true
    code-summary: "Click para ver el código"
    anchor-sections: true
    code-overflow: wrap
    fig-cap-location: top
lang: es
abstract-title: "Facultad de Ciencias Sociales e Historia UDP"
abstract: "[Click al repositorio](https://github.com/JDConejeros/SocioLab)"
---

::: logo-position
<!-- No modificar -->

<br> <strong></strong><br> 
:::

## Fenómenos en red 

![](files/robust_accion.png){width=900 height=500 fig-align="center"}

Robust Action and the Rise of the Medici, 1400-1434 de John F. Padgett y Christopher K. Ansell analiza cómo la familia Medici logró consolidar su poder en Florencia durante el Renacimiento. Utilizando un análisis de redes, los autores muestran que la clave del éxito de los Medici fue su capacidad para ocupar una posición central en múltiples redes (políticas, económicas y sociales), lo que les permitió manipular diferentes facciones y mantener su poder a través de una estrategia de "acción robusta." Puedes ver el artículo completo dando [click aquí](https://home.uchicago.edu/~jpadgett/papers/published/robust.pdf).


![](files/social_contagion.png){width=900 height=500 fig-align="center"}

![](files/social_contagion2.jpeg){width=900 height=900 fig-align="center"}

Puedes ver el artículo completo dando [click aquí](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3830455/).


![](files/surname.png){width=900 height=500 fig-align="center"}

![](files/surname2.png){width=900 height=500 fig-align="center"}

Puedes ver el artículo completo dando [click aquí](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3830455/).

## ¿Qué es el análisis de redes sociales? 

El análisis de redes es una estrategia metodológica que nos ayuda a estudiar las **estructuras subyacentes detrás de las relaciones que se dan entre los elementos de un sistema**, ya sean individuos, organizaciones, o cualquier unidad conectada con otra.

A continuación realizaremos una serie de análisis para entender la estructura de una red, para esto vamos a trabajar en `R` ya que tiene una serie de `packages` implementados y testeados con funciones para el análisis de redes. Por favor copia y pega el código en tu sesión de `RStudio`:


```{r}
#| warning: false
#| message: false
#| echo: true
#| results: 'hide'

# Limpiamos el enviroment y desactivamos la notación científica 
rm(list = ls())
options(scipen = 999)

# Cargamos las librerías que utilizaremos en el análisis
settings_packages <- function(packages){
  # Cargamos las tablas de datos
  for (i in packages) {
    if (i %in% rownames(installed.packages())) {
      library(i, character.only=TRUE)
    } else {
      install.packages(i)
      library(i, character.only = TRUE)
    }
  }
}

settings_packages(
  packages=c("rio", "dplyr", "broom", "ggplot2", "jtools", 
             "ggraph", "tidygraph", "GGally", "texreg",
             "igraph", "network", "sna", "ergm", # FUNDAMENTALES
             "ggmcmc", "patchwork", "DT", "ggnetwork", "visNetwork")
)
```


## Sobre los datos en red

Hoy vamos a utilizar unos datos de *Networked Pantheon*, una base de datos relacional que recopila biografías de personas globalmente famosas a lo largo de los últimos 5,500 años de historia. Esta base de datos complementa a *Pantheon 1.0*, la cual incluye información temporal, espacial, de género, y ocupacional de 11,341 figuras reconocidas a nivel mundial. Estas biografías están presentes en más de 25 versiones de Wikipedia en diferentes idiomas.La información sobre las relaciones entre estas figuras históricas es recopilada a partir de los hipervínculos presentes en los artículos de Wikipedia en inglés. Nosotros utilizamos solo un subconjunto de estos datos enfocados en el contexto latinoamericano. 

- **Nodos**: cada componente individual de una red. El tamaño de la red será el número total de nodos que la componen.

- **Enlaces (links)**: Interacciones entre los nodos de la red. Se representan como pares de nodos. Pueden ser dirigidos (nodo emisor -\> nodo receptor) o no dirigidos (nodo 1 \<-\> nodo 2). Un nodo puede tener enlaces con uno o varios otros nodos, o con ninguno.


Vamos a descargar los datos directamente a nuestra sesión de `RStudio` de la siguiente manera:

```{r}
#| warning: false
#| message: false
#| echo: true

nodos <- 
links <- 
```


- **Matriz de adyacencia**: es otra manera de representar una red, es una matriz que tiene en los ejes x e y los nodos de la red y cada valor de la matriz es 1 si existe ese enlace y 0 si no.



## Descripción de la estructura de una red 


- **Grados de los nodos**: representa el número de enlaces que tiene con otros nodos. Usualmente se denomina con $k_i$ el grado del i-esimo nodo de la red. En una red dirigida vamos a hablar de *in-degree* para la cantidad de enlaces que llegan dirigidos hacia un nodo, y de *out-degree* para la cantidad de enlaces que salen desde un nodo hacia otros.

- **Densidad**: es la proporción de enlaces existentes, se calcula como el número total de enlaces (orden de la red) sobre el número total de enlaces posibles (si todos los nodos estuvieran conectados con todos, es decir, un grafo completo).

- **Clustering**: es la densidad del nodo, es decir, el número de enlaces del nodo, dividido por el número total de enlaces que podrían formar los vecinos del nodo entre ellos.

- **Componentes**: una red puede ser dividida en sub redes o componentes cuando hay grupos de nodos que no son alcanzables por otro grupo. En una componente todos los nodos están enlazados a algún otro nodo.


## Formación de redes: modelos aleatorios

Usualmente nos interesa conocer los fenómenos que explican la formación de la estructura específica de una red. Para esto se ocupan los "Modelos Nulos" que son redes formadas de manera aleatoria que preservan alguna propiedad de la red que se quiere estudiar. Luego, se comparan métricas descriptivas de la red sintética con las métricas de la red que estamos estudiando (clustering y distancia), para saber si nuestra red pudo formarse con el mismo mecanismo usado para generar la red aleatoria. Hay diversos algoritmos para la creación de un modelo nulo:

- **Erdös Renyi**: la probabilidad es igual para cada enlace. En datos sociales los niveles de clusterización son generalmente mayores a los observados en las redes aleatorias de Erdos Renyi y existen nodos en la red con un grado muy alto.

- **Strogatz-Watts** (mundo pequeño): la distancia promedio de los nodos es pequeña y no cambia aunque se agreguen nuevos nodos a la red. 

- **Barabási** (modelo de libre escala): se basan en que la probabilidad de generar un enlace es mayor con nodos que tienen mayor grado, en estas redes aparecen naturalmente nodos con un grado muy alto.


## Modelos exponenciales de grafos aleatorios (ERGMs)

Esta metodología permite modelar redes para comprender y explicar las estructuras subyacentes de una red observada a partir de sus características estructurales y variables exógenas. Estos modelos permiten analizar la probabilidad de que un enlace se forme a partir de los atributos de la red y los nodos. Especificamos el modelo de la siguiente forma:
$$P(Y=y)=\frac{exp(\theta'g(y))}{k(\theta)}$$

Donde: 

- $Y$ es la variable aleatoria que representa el estado de la red con $y$ como la red observada.

- $g(y)$ es un vector de estadísticos descriptvos de la red (número de enlaces, tríadas, homofilia, etc.). Esto permite resumir las características estructurales de la red.

- $\theta$ es un vector de coeficientes asociados con las configuraciones estructurales de la red

- $k(\theta)$ es una constante de normalización que nos permite que la suma de las probabilidades sea igual a 1. 

Cada coeficiente $\theta$ en un ERGM refleja el aporte que tiene la característica en la probabilidad general de la red observada. 

- Parámetros **positivos** ($\theta>0$) indican que la configuración correspondiente (por ejemplo, más aristas o tríadas) es más frecuente en la red observada de lo que sería esperado por azar.

- Parámetros **negativos** ($\theta<0$) sugieren que la configuración es menos común en la red observada que lo esperado por azar.

La interpretación de los coeficientes en un ERGM es similar a la de los coeficientes en un modelo de regresión logística (*odds ratio*).


## Extra: Detección de comunidades 

**Girvan-Newman**: es una estrategia de detección de comunidades basada en la eliminación de los enlaces con mayor betweenness centrality (centralidad de intermediación). El algoritmo procede de manera iterativa, removiendo los nodos más "centrales" hasta que la red se descompone en comunidades más pequeñas.
$$C_B(e)=\sum_{s \neq t}\frac{\sigma(s,t|e)}{\sigma(s,t)}$$

Donde $\sigma(s,t)$ es el número total de caminos más cortos entre nodos $s$ y $t$, y $\sigma(s,t|e)$ es el número de esos caminos que pasan por el nodo $e$. Este algoritmo elimina de forma iterada los nodos con mayor $C_B(e)$ hasta que la red se divide en comunidades. 

- Ventaja: efectivo para redes simples e independiente del tamaño.

- Desventaja: en grafos grandes y ruidosos, tiende a sobrestimar el número de comunidades, ya que puede continuar eliminando aristas más allá del punto en que se debería detener.

**Label Propagation**: 

El algoritmo de propagación de etiquetas se basa en la idea de que los nodos adoptan la etiqueta más común entre sus vecinos. Este proceso se repite de forma iterada hasta que todas las etiquetas convergen hacia la identificación de comunidades.

Cada nodo $v$ tiene una etiqueta $L_v$ y en cada iteración se prueba que:
$$L_v=moda(L_u: u \in vecinos(v))$$

Donde la moda es el valor más frecuente. 

- Ventaja: rápido y escalable a redes de distintos tamaños.

- Desventaja: subestima el número de comunidades en redes muy ruidosas y puede no ser preciso en redes complejas.

**Louvain**

El algoritmo de Louvain es un método de optimización modular para la detección de comunidades, en otras palabras, maximiza la modularidad en una estructura jerárquica. La modularidad es una medida que compara la densidad de aristas dentro de las comunidades con la densidad de aristas en una red aleatoria.
$$Q=\frac{1}{2M}\sum_{ij} \biggl[ A_{ij} - \frac{k_ik_j}{2m} \biggr] \delta(c_i, c_j)$$

Donde $A_{ij}$ es el peso del enlace entre $i$ y $j$. $k_i$ y $k_j$ son los grados de los nodos, $m$ es el número de enlaces y $\delta(c_i, c_j)$ es 1 si $i$ y $j$ están en la misma comunidad y 0 en caso contrario.

- Ventaja: rápido y eficiente para diferentes tamaños de red.

- Desventaja: subestima el número de comunidades en redes grandes y ruidosas.

**Fast Greedy**

El algoritmo Fast Greedy también maximiza la modularidad ($Q$), pero lo hace de manera aglomerativa, fusionando nodos o grupos de nodos para aumentar la modularidad hasta que no se pueda mejorar más. El algoritmo busca iterativamente los pares de comunidades cuya fusión maximice la modularidad $Q$ y continua fusionando hasta que no se puede mejorar más. 

- Ventaja: rápido para redes de tamaño moderado.

- Desventaja: subestima el número de comunidades y su rendimiento se deteriora en redes con más de mil nodos.

**Infomap**

Infomap es un algoritmo que optimiza la compresión de un mapa de flujos de información en la red. Las comunidades se identifican minimizando la descripción de un paseo aleatorio en la red.

Se basa en la idea de minimización de longitud media de un código que describe un paseo aleatorio.
$$L(M) = q_{\curvearrowright} H(\mathcal{Q}) + \sum_{i=1}^{m} p_i^{\circlearrowright} H(\mathcal{P}_i)$$

Donde $H$ es la entropía de las distribuciones de salida y entrada de los nodos.

- Ventaja: identifica bien las comunidades en redes simples con menos de 1,000 nodos.

- Desventaja: puede no escalar bien en redes más grandes y complejas.

**Walktrap**

El algoritmo Walktrap se basa en la idea de que un paseo aleatorio en la red tenderá a quedar atrapado dentro de la misma comunidad. Utiliza estos paseos para agrupar nodos en comunidades.

La distancia entre los nodos se calcula a partir de los paseos aleatorios: 
$$d(i,j)=\sqrt{(r_i-r_j)'(r_i-r_j)}$$

Donde $r_i$ es el vector de probabilidades de visitar otros nodos después de un paseo de longitud $t$ desde $i$. 

- Ventaja: efectivo para encontrar el número correcto de comunidades en redes simples, independientemente del tamaño.

- Desventaja: Puede ser menos eficiente en redes complejas y ruidosas.


